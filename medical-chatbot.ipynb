{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31239,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Setup and Libraries\n#This cell installs the necessary optimization libraries for Quantization and Fine-tuning*/\n# Install optimization libraries\n!pip install -q auto-gptq optimum peft datasets accelerate bitsandbytes\n\nimport torch\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\nfrom sentence_transformers import SentenceTransformer, util","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Preprocessing (The \"X\")\n# This logic cleans the raw symptom data, removing underscores and grouping them by disease.\n# Load dataset\ndis_symp_df = pd.read_csv(\"/kaggle/input/disease-symptom-prediction/dataset.csv\")\n\n# Clean symptoms and group by disease\nsymptom_cols = [col for col in dis_symp_df.columns if col.startswith(\"Symptom\")]\ndis_symp_df[symptom_cols] = dis_symp_df[symptom_cols].replace(\"_\", \" \", regex=True)\n\n# Combine symptoms into a single string for RAG\ndis_symp_df[\"Symptoms\"] = dis_symp_df[symptom_cols].apply(lambda row: ', '.join([s for s in row if pd.notnull(s)]), axis=1)\ndis_symp_df_grouped = dis_symp_df.groupby(\"Disease\")[\"Symptoms\"].apply(lambda x: \", \".join(set(x))).reset_index()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# The RAG & Quantization Logic (The \"Z\" and \"Y\")\n# This part implements the 4-bit Quantization and Semantic Search you mentioned in your achievements.\n# Initialize Embeddings for Semantic Search\nembed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\ncorpus = dis_symp_df_grouped.apply(lambda row: f\"Disease: {row['Disease']}. Symptoms: {row['Symptoms']}\", axis=1).tolist()\ncorpus_embeddings = embed_model.encode(corpus, convert_to_tensor=True)\n\n# Configure 4-bit Quantization (QLoRA)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.float16\n)\n\n# Load Llama-2 Model\nmodel_name = \"NousResearch/Llama-2-7b-chat-hf\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fine-Tuning Setup (The \"Z\")\n# This implements the PEFT/LoRA parameters used to adapt the model to medical data.\n# Prepare model for PEFT/LoRA training\nmodel = prepare_model_for_kbit_training(model)\nlora_config = LoraConfig(\n    r=8, \n    lora_alpha=16, \n    target_modules=[\"q_proj\", \"v_proj\"], \n    lora_dropout=0.1, \n    bias=\"none\", \n    task_type=\"CAUSAL_LM\"\n)\nmodel = get_peft_model(model, lora_config)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}